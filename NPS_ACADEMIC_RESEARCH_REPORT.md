# NPS Survey Flow: Comprehensive Academic Research Report

## Table of Contents
1. [The Original NPS Methodology](#1-the-original-nps-methodology)
2. [Academic Critiques and Modifications to the Flow](#2-academic-critiques-and-modifications-to-the-flow)
3. [Contemporary Academic Developments (2020-2026)](#3-contemporary-academic-developments-2020-2026)
4. [Cross-Cultural NPS Flow Modifications](#4-cross-cultural-nps-flow-modifications)
5. [Psychometric Research on NPS](#5-psychometric-research-on-nps)
6. [The "Close the Loop" Step](#6-the-close-the-loop-step)

---

## 1. The Original NPS Methodology

### 1.1 Reichheld (2003) — The Foundational Article

**Citation:** Reichheld, F. F. (2003). "The One Number You Need to Grow." *Harvard Business Review*, 81(12), 46-54.
**URL:** https://hbr.org/2003/12/the-one-number-you-need-to-grow

**Key findings:**
- Based on two years of research linking survey questions to actual customer behavior (purchasing patterns and referrals) and company growth.
- The single question "How likely is it that you would recommend [company] to a friend or colleague?" on a 0-10 scale was identified as the strongest single predictor of growth across most industries studied.
- Responses are classified into three segments: **Promoters** (9-10), **Passives** (7-8), and **Detractors** (0-6).
- NPS = % Promoters - % Detractors (Passives are discarded from the calculation).
- Reichheld claimed this single metric was superior to complex customer satisfaction surveys.

**Original prescribed flow (as expanded in subsequent works):**
1. **Step 1 — The Ultimate Question:** "How likely are you to recommend [company/product] to a friend or colleague?" (0-10 scale)
2. **Step 2 — The Follow-up "Why" Question:** An open-ended question asking for the primary reason behind the score
3. **Step 3 — Close the Loop:** Company contacts the respondent (especially detractors) to address issues

**Flow design implication:** Reichheld deliberately designed a minimalist two-question survey (score + open-ended why) to maximize response rates and actionability. The brevity was considered a feature, not a limitation.

---

### 1.2 Reichheld (2006/2011) — The Ultimate Question 2.0

**Citation:** Reichheld, F. F. & Markey, R. (2011). *The Ultimate Question 2.0: How Net Promoter Companies Thrive in a Customer-Driven World.* Harvard Business Press.

**Key findings:**
- Expanded the NPS from a metric into a full management system ("Net Promoter System").
- Emphasized the operational importance of the follow-up open-ended question and the "close the loop" process.
- Distinguished between **relationship NPS** (periodic brand-level surveys) and **transactional NPS** (triggered after specific interactions).
- Introduced the concept of "inner loop" (frontline employee follow-up with individual customers) and "outer loop" (systemic organizational improvements).

**Flow design implication:** The prescribed flow became: (1) Score question, (2) Open-ended "why" question, (3) Immediate frontline follow-up with detractors, (4) Systemic analysis and organizational action.

---

### 1.3 Reichheld (2021) — Net Promoter 3.0

**Citation:** Reichheld, F. F., Darnell, D., & Burns, M. (2021). "Net Promoter 3.0." *Harvard Business Review*, 99(6), 38-47.
**URL:** https://hbr.org/2021/11/net-promoter-3-0

**Key findings:**
- Acknowledged that NPS had been "gamed and misused in ways that hurt its credibility" with unaudited, self-reported scores.
- Introduced the **Earned Growth Rate** as a complementary, auditable metric based on accounting data.
- Earned Growth Rate distinguishes between "earned" customers (from referrals/recommendations) and "bought" customers (from advertising/promotions).
- Argued that auditable financial metrics should complement self-reported NPS to prevent manipulation.

**Flow design implication:** Suggests that the NPS survey flow should be supplemented with behavioral/transactional data validation rather than relying solely on survey responses.

---

## 2. Academic Critiques and Modifications to the Flow

### 2.1 The 0-10 Scale Itself

#### Grisaffe (2007) — Conceptual Critique of Arbitrary Cutoffs

**Citation:** Grisaffe, D. B. (2007). "Questions about the ultimate question: Conceptual considerations in evaluating Reichheld's Net Promoter Score (NPS)." *Journal of Consumer Satisfaction, Dissatisfaction and Complaining Behavior*, 20, 36-53.

**Key findings:**
- The break-points between NPS segments (Detractors 0-6, Passives 7-8, Promoters 9-10) appear arbitrary with little information on whether they are context-specific or empirically derived.
- Because NPS is calculated based on only two of three categories (Promoters and Detractors), a significant portion of information from Passives is lost.
- NPS is not sufficient as an approach to customer loyalty measurement and management.

**Flow design implication:** The traditional 0-10 NPS scale with its 3-segment classification may discard actionable information. Survey flows could benefit from treating the full 11-point distribution rather than collapsing it into categories.

---

#### Bendle & Bagga (2016) — Statistical Critique of NPS

**Citation:** Bendle, N. T. & Bagga, C. K. (2016). "The Metrics That Marketers Muddle." *MIT Sloan Management Review*, 57(3), 73-82.
**URL:** https://sloanreview.mit.edu/article/the-metrics-that-marketers-muddle/

Also: Bendle, N. T. & Bagga, C. K. (2016). "Should You Use Net Promoter Score as a Metric?" *MIT Sloan Management Review*.
**URL:** https://sloanreview.mit.edu/article/should-you-use-net-promoter-score-as-a-metric/

**Key findings:**
- NPS was described as "the one number you need to know to grow" and associated with "profitable growth," but the supporting evidence relied on revenue growth rather than bottom-line (profit) growth.
- It is easy to imagine how to increase NPS while destroying even top-line growth (e.g., by firing difficult but profitable customers).
- NPS is one of five widely-used marketing metrics that are regularly misunderstood and misused, and this confusion undermines the marketing discipline's reputation for delivering results.
- Most executives believe there is "strong scientific support for the claim that NPS is more effective than all other customer satisfaction metrics" — a belief not supported by the evidence.

**Flow design implication:** Organizations should not optimize their entire survey flow and business processes around a single metric that has limited scientific support for its claimed superiority. NPS should be one metric among several, not the sole focus.

---

#### Eskildsen & Kristensen (2011) — NPS as Unreliable Measure

**Citation:** Eskildsen, J. K. & Kristensen, K. (2011). "Is the Net Promoter Score a reliable performance measure?" *Conference paper, referenced in multiple subsequent publications.*

**Key findings:**
- NPS is an inefficient and unreliable measure of customer loyalty.
- The arbitrary cut-off points mean the "Detractor" category includes customers who answer the LTR question favorably (e.g., a 6 on a 0-10 scale).
- Proposed calculating the **average of all LTR scores** rather than categorizing them, finding this provides better insight.
- More recent research building on this found that NPS (Average) offers the highest explanatory power for customer satisfaction, followed by NPS (Top 3).

**Flow design implication:** The scoring and display step could use mean scores rather than the traditional promoter-minus-detractor formula, yielding more statistically stable results.

---

#### Fisher (2019) — Critical Statistical Review

**Citation:** Fisher, N. I. & Kordupleski, R. E. (2019). "Good and bad market research: A critical review of Net Promoter Score." *Applied Stochastic Models in Business and Industry*, 35(1), 138-151.
**URL:** https://onlinelibrary.wiley.com/doi/abs/10.1002/asmb.2417

**Key findings:**
- NPS possesses few of the characteristics regarded as highly desirable in a high-level market research metric.
- The counting method used to calculate NPS introduces additional variation in scores compared to mean average likelihood-to-recommend scores. This additional variation occurs both across brands in a study, as well as for the same brand over survey waves.
- NPS requires larger sample sizes than customer feedback metrics based on average calculations.
- Presents a better solution than NPS for understanding what customers value.

**Flow design implication:** If using NPS in a survey flow, designers should be aware that the metric is inherently noisier than mean-based alternatives and requires substantially larger sample sizes to achieve stable results.

---

### 2.2 Question Wording and Framing Effects

#### Schlosser (2024) — NPS Question Design Flaws

**Citation:** Schlosser, A. (2024). "To recommend or not recommend is the question: Does NPS predict word-of-mouth?" *International Journal of Market Research*, 66(2-3).
**DOI:** https://doi.org/10.1177/14707853231186309

**Key findings:**
- The NPS question is **double-barreled** by asking in a single question for likelihood to recommend to friends AND likelihood to recommend to colleagues.
- NPS focuses only on recommendations and thus **ignores consumers' likelihood to spread negative WOM**.
- NPS **ignores online WOM**, which often involves recommendations to strangers rather than friends or colleagues.
- Examines how consumers interpret NPS, the factors that influence these interpretations, and how these factors affect NPS' ability to predict negative WOM, online WOM, satisfaction, loyal behavior, and WOM in general.

**Flow design implication:** The traditional NPS question should be unbundled. Separate questions for friend recommendations, colleague recommendations, and online sharing would yield more valid data. Adding a question about negative WOM propensity would capture the full picture.

---

#### Pechter & Kuusik (2024) — Recency Effects on NPS Responses

**Citation:** Pechter, J. & Kuusik, A. (2024). "NPS from the customer's perspective: The influence of the recent experience." *International Journal of Market Research*, 66(2-3).
**DOI:** https://doi.org/10.1177/14707853231214188

**Key findings:**
- Analyzed how respondents answer the LTR question in different pre-described and validated situations, presenting 1,000 participants with situation descriptions consisting of previous and recent experiences.
- Respondents do not always give a high score for good experiences, and a low score for bad experiences.
- With a high number of respondents, the different answering approaches even out, and the NPS results are higher for good than for bad experiences overall.

**Flow design implication:** The timing of NPS surveys matters enormously. Recent experiences disproportionately color responses. Survey flows should consider whether to measure relationship NPS (less susceptible to recency) vs. transactional NPS (deliberately capturing recency).

---

### 2.3 Follow-Up Question Design

#### Bettencourt & Houston (2024) — The Untested Assumption

**Citation:** Bettencourt, L. A. & Houston, M. B. (2024). "The Untested Assumption: Can a Net Promoter Study Be Used to Improve Net Promoter Score?" *International Journal of Market Research*, 66(2-3).
**DOI:** https://doi.org/10.1177/14707853231198780

**Key findings:**
- The critical, but untested, assumption in NPS use is that insights from open-ended customer comments can be used to set improvement priorities that will increase future NPS ratings.
- Results reveal that this assumption is **questionable**, with **low to moderate convergence** between open-ended priorities from a net promoter study and predictors of the standard 'likely-to-recommend' NPS question.
- Convergence between customers' open-ended priorities and their stated and derived priorities from closed-ended responses is **higher for detractors and passives than promoters**, but still only moderate.
- Firms should ask customers, especially detractors, to identify specific areas for improvement.
- Firms should **supplement open-ended customer feedback with closed-ended questions** to get customers' performance ratings in relation to their specific needs.

**Flow design implication:** This is critical for survey flow design. The traditional NPS flow of "score + open-ended why" is insufficient. A modified flow should include: (1) NPS score, (2) Conditional closed-ended driver questions (what aspects matter most?), (3) Open-ended "why" question. The closed-ended questions are essential because open-ended responses alone do not reliably identify what actually drives the score.

---

### 2.4 Response Option Labeling

#### Menold (2020) — Verbal vs. Numeric Rating Scales

**Citation:** Menold, N. (2020). "Rating-Scale Labeling in Online Surveys: An Experimental Comparison of Verbal and Numeric Rating Scales with Respect to Measurement Quality and Respondents' Cognitive Processes." *Sociological Methods & Research*, 49(1), 79-107.
**DOI:** https://doi.org/10.1177/0049124117729694

**Key findings:**
- Eye-tracking study: respondents needed more fixations and more time to endorse a category when a rating scale had numeric labels only.
- "It is more difficult for the respondents to map their responses onto the response categories with the numeric than with the verbal rating scales."
- Cross-sectional reliability was lower and some validity hypotheses could not be supported when numeric rating scales were used.
- The research advises **against using only numeric labels** for rating scales in web surveys.

**Flow design implication:** The standard NPS scale (0-10 with only endpoint labels "Not at all likely" and "Extremely likely") may be suboptimal. Fully labeled verbal scales produce better measurement quality, though they impose higher cognitive load.

---

#### Classification of Response Scale Characteristics (2017)

**Citation:** DeCastellarnau, A. (2018). "A classification of response scale characteristics that affect data quality: a literature review." *Quality & Quantity*, 52(4), 1523-1559.
**DOI:** https://doi.org/10.1007/s11135-017-0533-4

**Key findings:**
- The majority of research shows that labelling all points in the scale has a positive impact on reliability.
- However, conflicting evidence exists: some studies find higher validity when only endpoints are labeled.
- Fully labeled 5-point scales are recommended to reduce the effect of visual cues.
- Number of scale points affects data quality: expanding from 5 to 7 points showed no further improvement in reducing extreme responding.

**Flow design implication:** The NPS 11-point (0-10) numeric-only scale may be too granular. A fully-labeled 5-point or 7-point scale could achieve comparable discriminatory power with better measurement properties.

---

### 2.5 Order Effects

#### Thau (2021) / van de Walle & van Ryzin — Question Order in Satisfaction Surveys

**Citation:** Thau, M. (2021). "Question order bias revisited: A split-ballot experiment on satisfaction with public services among experienced and professional users." *Public Administration*, 99(1), 132-146.
**DOI:** https://doi.org/10.1111/padm.12688

Also: van de Walle, S. & van Ryzin, G. (2011). "The Order of Questions in a Survey on Citizen Satisfaction with Public Services." *Public Administration*, 89(4), 1436-1450.

**Key findings:**
- Asking about overall satisfaction **before** any specific service ratings **lowers** overall user satisfaction, compared to the reverse order.
- The correlations between specific ratings and overall satisfaction, and thus the identification of key drivers of overall satisfaction, are **highly sensitive to question order**.
- The order of questions has important effects on reported satisfaction with specific public services as well as overall citizen satisfaction.

**Flow design implication:** If NPS (a global recommendation question) is placed first, before any specific attribute questions, it may yield different scores than when placed after specific questions. If the goal is to identify drivers, the NPS question should be placed AFTER specific attribute questions. If the goal is an unbiased overall metric, it should be placed FIRST (before specific questions prime particular attributes).

---

### 2.6 Scale Direction (Ascending vs. Descending)

#### Chyung (2018) / Hartley & Betts — Evidence-Based Scale Direction

**Citation:** Chyung, S. Y. et al. (2018). "Evidence-Based Survey Design: The Use of Ascending or Descending Order of Likert-Type Response Options." *Performance Improvement*, 57(3), 9-16.
**DOI:** https://doi.org/10.1002/pfi.21800

Also: Research on descending vs. ascending effects, including: "Descending Beats Ascending: Effects of Order on the Likert Scale on Consumer Ratings." (2023). ResearchGate.

**Key findings:**
- Scale direction effects produce **primacy effects** across telephone, face-to-face, and web survey modes — respondents are drawn to the first plausible option.
- The average reported life satisfaction is **0.7 points lower** when a descending (high-to-low) 11-point scale is used compared to ascending (low-to-high).
- Correlations of the ascending scale are **significantly stronger**, indicating **higher validity** for ascending (low-to-high) formats.
- Primacy effects are even stronger when scales start with the positive end (descending order).

**Flow design implication:** The standard NPS scale runs 0 (left) to 10 (right), which is ascending order. This is empirically supported as the higher-validity direction. Reversing the scale (10 to 0) would reduce validity and introduce additional bias.

---

### 2.7 Open-Ended vs. Closed-Ended Follow-Up Questions

#### Hadler (2025) — Effects of Open-Ended Probes on Closed Questions

**Citation:** Hadler, P. (2025). "The Effects of Open-Ended Probes on Closed Survey Questions in Web Surveys." *Sociological Methods & Research*, 54(1).
**DOI:** https://doi.org/10.1177/00491241231176846

**Key findings:**
- Embedding open-ended probes resulted in **higher levels of survey break off**, as well as increased **backtracking and answer changes** to previous questions.
- However, in most cases, there was **no impact of open-ended probes on the cognitive processing of and response to** the preceding closed survey questions.
- Open-ended questions require greater cognitive effort, and the quality of data obtained correlates with respondent background variables (literacy, education).
- Open-ended questions yield richer information but have higher **non-response rates** and may **discourage participants with lower literacy** levels.

**Flow design implication:** The NPS open-ended "why" question may increase dropout compared to a survey with only the numeric score question. To mitigate this: (a) make the open-ended question optional, (b) use a text box with a prompt rather than a blank field, (c) provide option to select from common reasons first, then elaborate.

---

## 3. Contemporary Academic Developments (2020-2026)

### 3.1 Modified NPS Flows: Adding Emotions alongside NPS

#### Muller, Seiler & Volkle (2024) — NPS Supplemented with Emotional Metrics

**Citation:** Muller, S., Seiler, R., & Volkle, M. (2024). "Should Net Promoter Score be supplemented with other customer feedback metrics? An empirical investigation of Net Promoter Score and emotions in the mobile phone industry." *International Journal of Market Research*, 66(2-3).
**DOI:** https://doi.org/10.1177/14707853231219648

**Key findings:**
- A combination of NPS and NEV (Net Emotional Value) leads to a **better explanation of two out of three outcome variables** compared to using NPS only or NEV only.
- Emotional profiles and driver analyses can be used to identify the most relevant emotions of Detractors, Passives, and Promoters.
- NPS alone does not capture the emotional dimension of customer experience.

**Flow design implication:** Survey flows should add an emotion-capture step after the NPS score. This could be: (1) NPS score, (2) Emotional response selection (e.g., from a validated emotion list), (3) Open-ended follow-up. The emotional data provides richer segmentation and more actionable insights.

---

### 3.2 Comparing NPS with Other Customer Feedback Metrics

#### De Haan, Verhoef & Wiesel (2015) — Predictive Ability of CFMs

**Citation:** De Haan, E., Verhoef, P. C., & Wiesel, T. (2015). "The predictive ability of different customer feedback metrics for retention." *International Journal of Research in Marketing*, 32(2), 195-206.

**Key findings:**
- Data from customers of 93 firms across 18 industries.
- **Top-2-box customer satisfaction performs best** for predicting customer retention; focusing on the extremes is preferable to using the full scale.
- The best CFM differs depending on industry and the unit of analysis.
- **Combining CFMs along with simultaneously investigating multiple dimensions of the customer relationship improves predictions** even further.
- There is no single best customer feedback metric in explaining all outcome variables.

**Flow design implication:** A multi-metric survey flow outperforms single-metric approaches. The optimal flow includes NPS + CSAT + CES (or a subset), with industry-specific weighting. This argues for a 3-4 question core survey rather than a single NPS question.

---

### 3.3 AI/NLP-Enhanced Follow-Up Analysis

#### Lewis & Mehmet (2020) — NPS Sentiment Analysis

**Citation:** Lewis, C. & Mehmet, M. (2020). "Does the NPS reflect consumer sentiment? A qualitative examination of the NPS using a sentiment analysis approach." *International Journal of Market Research*, 62(1), 9-21.
**DOI:** https://doi.org/10.1177/1470785319863623

**Key findings:**
- Collected NPS ratings from consumers and asked them to explain their rating in an open question, then analyzed the sentiment of responses with natural language data analysis.
- Found a **high correlation between NPS ratings and sentiment** of open-ended responses at an overall level.
- However, **caution should be used when classifying participants into detractors, passives, and promoters** — sentiment analysis reveals more nuance within these categories.

**Flow design implication:** Open-ended follow-up responses can be reliably analyzed via NLP/sentiment analysis to validate and enrich NPS scores. Automated sentiment scoring of the "why" question can serve as a quality check on the numeric score.

---

#### Tarnowska & Ras (2019) — ML-Based NPS Recommender System

**Citation:** Tarnowska, K. & Ras, Z. W. (2019). "NPS-Based Recommender System for Improving Net Promoter Score of a Company." *Research project, University of North Carolina at Charlotte.* Published across multiple venues including Springer.
**URL:** https://www.researchgate.net/project/Recommender-System-for-Improving-Net-Promoter-Score-of-a-Company

Also: Tarnowska, K., Ras, Z., & Daniel, L. (2020). "NLP-Based Customer Loyalty Improvement Recommender System (CLIRS2)."

**Key findings:**
- Developed a data-driven recommender system using machine learning and text mining (classification, clustering, rules mining) to identify NPS drivers from approximately 400,000 records across 38 companies.
- The system uses **sentiment analysis** to generate recommendations for customer loyalty improvement, transforming open-ended text data into structured form for mining.
- Knowledge is extracted not only from a particular business's customers but also from customers of **semantically similar businesses with higher NPS scores**, enabling cross-company learning.
- The system generates actionable recommendations in a format managers can use directly.

**Flow design implication:** Modern NPS survey flows can feed into automated ML pipelines that identify key drivers and generate improvement recommendations. The open-ended "why" question becomes the raw material for machine learning models, making it even more important to collect high-quality text responses.

---

#### End-to-End NLP Solution for NPS (2023)

**Citation:** Research on "An End-to-End Solution for Net Promoter Score Estimation and Explanation from Social Media Using Natural Language Processing." ResearchGate, 2023.

**Key findings:**
- Developed processes for sentiment analysis on user comments, estimating product information based on text semantics, grouping and tagging user comments for text discovery, and NPS explanation.
- Used deep learning-based text feature extraction methods (BERT, Word2Vec, Doc2Vec, FastText, GloVe) for binary classification of conversation transcripts.
- Demonstrated that transformer-based models like BERT can predict NPS categories from unstructured text.

**Flow design implication:** The "why" open-ended question can now be automatically categorized using modern NLP, enabling real-time adaptive survey flows where the system identifies themes and can ask targeted follow-up questions.

---

### 3.4 Visual/UX Design of the Scale

#### Bosch, Revilla, DeCastellarnau & Weber (2019) — Slider vs. Radio Button

**Citation:** Bosch, O. J., Revilla, M., DeCastellarnau, A., & Weber, W. (2019). "Measurement Reliability, Validity, and Quality of Slider Versus Radio Button Scales in an Online Probability-Based Panel in Norway." *Social Science Computer Review*, 37(1), 119-139.
**DOI:** https://doi.org/10.1177/0894439317750089

**Key findings:**
- If smartphone respondents represent a nonnegligible part of the sample, sliders or radio button scales lead to **quite similar measurement quality**.
- If there are no smartphone respondents, sliders can be used, but the marker should be placed initially in the middle rather than on the left side.
- In practice, **there is no need to shift from radio buttons to sliders** since quality is not highly improved.

---

#### Funke, Reips & Thomas (2011) — Sliders vs. VAS vs. Buttons

**Citation:** Referenced in multiple survey methodology works. Published circa 2011, with extended follow-up studies.

**Key findings:**
- Slider scales **negatively affect response rate** (especially on mobile devices), sample composition, the distribution of values, and increase response times.
- Slider bars showed lower mean scores and more nonresponses than buttons.
- **VAS and radio buttons can be used without negative side effects**, even on touch screen devices like smartphones.
- Overall recommendation: **avoid slider scales**.

**Flow design implication:** NPS surveys should use **radio buttons or discrete clickable number buttons**, not sliders. On mobile, discrete buttons are clearly superior for response rate and data quality.

---

#### Hu (2020) — Horizontal vs. Vertical Orientation

**Citation:** Hu, J. (2020). "Horizontal or Vertical? The Effects of Visual Orientation of Categorical Response Options on Survey Responses in Web Surveys." *Social Science Computer Review*, 38(6), 741-759.
**DOI:** https://doi.org/10.1177/0894439319834296

**Key findings:**
- Horizontal orientation is more **burdensome** than vertical orientation on the web.
- Horizontal orientation requires more hand/eye movement and is more effortful.
- The effects of cognition capacity interact with orientation on both response burden and response quality.
- **Vertical orientation** for categorical response options is recommended in general.

**Flow design implication:** The traditional horizontal NPS scale (0 through 10 in a row) may not be optimal, especially for older respondents or those with lower cognitive capacity. Vertical layouts may reduce burden, though they require more scroll space.

---

### 3.5 Conversational / Chatbot-Based Survey Delivery

#### Kim, Lee & Gweon (2019) / Xiao et al. (2023) — Chatbot vs. Traditional Surveys

**Citation:** Kim, S., Lee, J., & Gweon, G. (2019). "Comparing Data from Chatbot and Web Surveys: Effects of Platform and Conversational Style on Survey Response Quality." *Proceedings of the ACM CHI Conference on Human Factors in Computing Systems.*

Also: Xiao, Z. et al. (2023). "Comparing Chatbots and Online Surveys for (Longitudinal) Data Collection." *Published on ResearchGate.*

**Key findings:**
- Chatbot surveys produce **more differentiated responses** and are **less likely to induce satisficing behavior**, resulting in higher-quality data.
- Chatbots elicit **significantly better-quality text data** (open-ended responses) in terms of informativeness, relevance, specificity, and clarity.
- About half of participants **preferred the chatbot** over conventional surveys, citing personalized conversation flow and supportive tone.
- However, a longitudinal study found that chatbot surveys generate **lower word entries** in open-ended questions compared to web surveys, and this difference **tends to increase** over time.
- When users were asked to disclose sensitive information, they perceived a **formal style** as more competent and appropriate.
- Dropout rates were similar between chatbot and web surveys (~8-10%).

**Flow design implication:** NPS surveys could be delivered via conversational interfaces (chatbots) to improve engagement and open-ended response quality. However, the conversational style should be professional/formal rather than casual, and designers should be aware that text response length may decrease over repeated interactions.

---

### 3.6 Perceived Value and NPS Drivers

#### Maubisson, Mencarelli & Riviere (2024) — Value Sources Distinguishing NPS Categories

**Citation:** Maubisson, L., Mencarelli, R., & Riviere, A. (2024). "An empirical study of the relationship between perceived value and the net promoter score: Application to specialized retail chains." *International Journal of Market Research*, 66(2-3).
**DOI:** https://doi.org/10.1177/14707853231219422

**Key findings:**
- Studied 893 customers from two specialized retail chains to examine relations between value sources and customer categorization as Detractors, Passives, or Promoters.
- Different value dimensions (functional, emotional, social, epistemic) distinguish the three NPS segments differently.
- Understanding which value sources differentiate segments enables more targeted improvement strategies.

**Flow design implication:** Follow-up questions in NPS surveys could be structured around value dimensions rather than generic "why" questions — e.g., asking about functional value (quality, price), emotional value (feelings, experience), and social value (status, belonging).

---

### 3.7 Optimal Number of Scale Points

#### Preston & Colman (2000) — Optimal Response Categories

**Citation:** Preston, C. C. & Colman, A. M. (2000). "Optimal number of response categories in rating scales: Reliability, validity, discriminating power, and respondent preferences." *Acta Psychologica*, 104(1), 1-15.

**Key findings:**
- Two-, three-, and four-point scales performed relatively poorly on indices of reliability, validity, and discriminating power.
- Indices were **significantly higher for scales with more categories up to about 7** points.
- **Internal consistency did not differ significantly** between scales beyond 7 points.
- **Test-retest reliability tended to decrease** for scales with more than 10 response categories.
- Respondent preferences were highest for the 10-point scale, closely followed by 7-point and 9-point scales.
- The literature generally supports the use of **5 to 9 points**.

**Flow design implication:** The NPS 11-point (0-10) scale exceeds the psychometric optimum of 5-9 points and may actually reduce test-retest reliability. While respondents prefer larger scales, psychometric properties plateau or decline beyond 7 points. A 7-point NPS variant might achieve similar discrimination with better reliability.

---

## 4. Cross-Cultural NPS Flow Modifications

### 4.1 Cultural Response Styles and NPS

#### Seth, Scott, Svihel & Stephen (2016) — Mystery of Low NPS in East Asia

**Citation:** Referenced as: "Solving the Mystery of Consistent Negative/Low Net Promoter Score (NPS) in Cross-Cultural Marketing Research." *Asia Marketing Journal*, 17(4), 43-61.
**URL:** https://www.researchgate.net/publication/297604171

**Key findings:**
- Japanese and Korean respondents may have a **positive attitude** toward a company but provide **low NPS scores** because they would not risk ruining relationships with friends by making a recommendation.
- In the NPS system these people are labeled as "detractors" when they are actually **"ambivalent customers"** — satisfied but culturally unwilling to recommend.
- A psychological analysis of NPS results reveals that the negative NPS scores often found in Japan and Korea reflect **cultural relationship preservation norms**, not dissatisfaction.

**Flow design implication:** In collectivist cultures, the NPS question fundamentally measures something different — it captures cultural reluctance to recommend rather than dissatisfaction. Survey flows for these markets should either: (a) reframe the question (e.g., "How satisfied are you with [product]?" rather than recommendation intent), (b) use culturally calibrated scoring bands, or (c) supplement NPS with direct satisfaction measures.

---

### 4.2 Cross-National Response Style Patterns

#### Harzing (2006) — 26-Country Response Style Study

**Citation:** Harzing, A.-W. (2006). "Response styles in cross-national survey research: A 26-country study." *International Journal of Cross-Cultural Management*, 6(2), 243-266.
**DOI:** https://doi.org/10.1177/1470595806066332

**Key findings:**
- Country-level characteristics such as **power distance, collectivism, uncertainty avoidance and extraversion** all significantly influence response styles.
- **Extreme Response Style (ERS)** — tendency to use endpoints — varies systematically by culture.
- **Midpoint Response Style (MRS)** — tendency to select the middle option — is more common in East Asian cultures.
- These patterns are persistent and affect all rating scale data.

**Flow design implication:** NPS scores are not directly comparable across countries. Survey flows used internationally should include response style controls or calibration questions. Scale compression (where respondents cluster in the middle of the scale) artificially depresses NPS in some cultures.

---

#### Beuthner, Friedrich, Herbes & Ramme (2018) — Mexican vs. South Korean Response Styles

**Citation:** Beuthner, C., Friedrich, M., Herbes, C., & Ramme, I. (2018). "Examining survey response styles in cross-cultural marketing research: A comparison between Mexican and South Korean respondents." *International Journal of Market Research*, 60(1), 33-48.
**DOI:** https://doi.org/10.1177/1470785318762015

**Key findings:**
- Respondents in Mexico showed a bias toward **extreme responses** (selecting 0 or 10).
- Respondents in South Korea showed a response bias toward **midpoint values** (selecting 5, 6, 7).
- ERS for Hispanic respondents disappeared when 10-point Likert scales were used.
- 5-point scales reduced extreme responding compared with 3-point scales, but expanding to 7 points offered no further improvement.

**Flow design implication:** The 0-10 NPS scale may produce artificially polarized results in some Latin American cultures (inflating both Promoters and Detractors) while compressing scores in East Asian cultures (inflating Passives). A 5-point or 7-point scale may produce more culturally comparable results.

---

## 5. Psychometric Research on NPS

### 5.1 Single-Item Reliability and Validity

#### Bergkvist & Rossiter (2007) — Single-Item vs. Multi-Item Predictive Validity

**Citation:** Bergkvist, L. & Rossiter, J. R. (2007). "The Predictive Validity of Multiple-Item versus Single-Item Measures of the Same Constructs." *Journal of Marketing Research*, 44(2), 175-184.
**DOI:** https://doi.org/10.1509/jmkr.44.2.175

**Key findings:**
- For constructs consisting of a **concrete singular object and a concrete attribute** (like brand attitude), there is **no difference in predictive validity** between multi-item and single-item measures.
- For such "doubly concrete" constructs, single-item measures should be used.

**Flow design implication:** The NPS question, being a single-item measure of a relatively concrete construct (likelihood to recommend), may have adequate predictive validity. However, "likelihood to recommend" is arguably NOT a doubly concrete construct — it involves a hypothetical future behavior with multiple contextual dimensions, which weakens the applicability of Bergkvist & Rossiter's finding to NPS.

---

#### Diamantopoulos, Sarstedt, Fuchs, Wilczynski & Kaiser (2012) — Guidelines for Scale Choice

**Citation:** Diamantopoulos, A., Sarstedt, M., Fuchs, C., Wilczynski, P., & Kaiser, S. (2012). "Guidelines for choosing between multi-item and single-item scales for construct measurement: A predictive validity perspective." *Journal of the Academy of Marketing Science*, 40(3), 434-449.
**DOI:** https://doi.org/10.1007/s11747-011-0300-3

**Key findings:**
- The predictive validity of single items **varies considerably** across different constructs and stimulus objects.
- Under most conditions typically encountered in practical applications, **multi-item scales clearly outperform single items**.
- A comprehensive simulation study identified the influence of different factors on the predictive validity of single versus multi-item measures.

**Flow design implication:** The single NPS question is likely to underperform compared to a short multi-item loyalty scale. Survey flows could maintain the NPS question for benchmarking comparability while adding 2-3 additional items to improve measurement quality.

---

### 5.2 Comprehensive Validity Assessment

#### Keiningham, Cooil, Andreassen & Aksoy (2007) — Longitudinal NPS Examination

**Citation:** Keiningham, T. L., Cooil, B., Andreassen, T. W., & Aksoy, L. (2007). "A Longitudinal Examination of Net Promoter and Firm Revenue Growth." *Journal of Marketing*, 71(3), 39-51.
**DOI:** https://doi.org/10.1509/jmkg.71.3.039

**Key findings:**
- Using longitudinal data from 21 firms and 15,500+ interviews from the Norwegian Customer Satisfaction Barometer.
- **Failed to replicate** assertions regarding the "clear superiority" of NPS compared with other measures.
- Recommend intention alone will **not suffice as a single predictor** of customers' future loyalty behavior.
- A **multiple-indicator model** instead of a single predictor model performs better in predicting customer recommendations and retention.

**Flow design implication:** The NPS question alone is insufficient. Survey flows should include at least 2-3 loyalty indicators (e.g., repurchase intention, satisfaction, recommendation likelihood) for reliable prediction of actual customer behavior.

---

#### Jaramillo, Deitz, Hansen & Babakus (2024) — Construct and Predictive Validity

**Citation:** Jaramillo, S., Deitz, G., Hansen, J. D., & Babakus, E. (2024). "Taking the measure of net promoter score: An assessment of construct and predictive validity." *International Journal of Market Research*, 66(2-3).
**DOI:** https://doi.org/10.1177/14707853231213274

**Key findings:**
- NPS scores correspond to reported word-of-mouth exposure for **most, but not all**, product categories.
- NPS responses are **invariant across demographic groupings** (supporting measurement fairness).
- Both customer satisfaction and NPS are **significant predictors** of financial performance, but neither alone is clearly superior.

**Flow design implication:** NPS has acceptable construct validity for most product categories but should not be assumed to work universally. Survey flows may need category-specific validation.

---

#### Morgan & Rego (2006) — NPS vs. Satisfaction for Predicting Business Performance

**Citation:** Morgan, N. A. & Rego, L. L. (2006). "The Value of Different Customer Satisfaction and Loyalty Metrics in Predicting Business Performance." *Marketing Science*, 25(5), 426-439.

**Key findings:**
- **Average satisfaction scores have the greatest value** in predicting future business performance.
- Top-2-box satisfaction scores also have good predictive value.
- **Metrics based on recommendation intentions (Net Promoters) and behavior (average number of recommendations) have little or no predictive value** for business performance.
- Results clearly indicate that "recent prescriptions to focus customer feedback systems and metrics solely on customers' recommendation intentions and behaviors are misguided."

**Flow design implication:** This is one of the strongest academic challenges to NPS-only survey flows. The evidence suggests that a satisfaction question may be more predictive of business outcomes than the NPS question. Survey flows should include at minimum a customer satisfaction item alongside or instead of NPS.

---

#### Keiningham, Aksoy, Cooil & Andreassen (2008) — Holistic Examination

**Citation:** Keiningham, T. L., Aksoy, L., Cooil, B., & Andreassen, T. W. (2008). "A Holistic Examination of Net Promoter." *Journal of Database Marketing & Customer Strategy Management*, 15(2), 79-90.
**DOI:** https://doi.org/10.1057/dbm.2008.4

**Key findings:**
- Tested claims that NPS is the single most reliable indicator of a company's ability to grow and that NPS is superior to customer satisfaction.
- **Neither claim was supported** by the data.
- Examined variables including share of wallet, recommend intention, repurchase intention, overall satisfaction, worth what paid, expectations, and brand preference.
- A single-metric NPS model was outperformed by both **dual-metric and multi-metric models**.

**Flow design implication:** Corroborates the case for multi-metric survey flows. At minimum, adding repurchase intention and overall satisfaction to the NPS question produces superior prediction of actual customer behavior.

---

#### Romaniuk, Nguyen & East (2011) — Self-Reported vs. Actual Recommendation Behavior

**Citation:** Romaniuk, J., Nguyen, C., & East, R. (2011). "The Accuracy of Self-Reported Probabilities of Giving Recommendations." *International Journal of Market Research*, 53(4), 507-521.
**DOI:** https://doi.org/10.2501/IJMR-53-4-507-521

**Key findings:**
- Respondents are **better at predicting when they won't give a recommendation** than when they will.
- The main reason for inaccuracy was an **over-reliance on past circumstances**.
- Stated intention to recommend does not accurately predict actual recommendation behavior.

**Flow design implication:** The NPS question captures intention, not behavior. Survey flows could be improved by also measuring past recommendation behavior ("Have you recommended [company] in the last 6 months?") alongside future intention.

---

### 5.3 Cognitive Load and Survey Length

#### Brosnan, Grun & Dolnicar (2021) — Cognitive Load Reduction in Questionnaire Design

**Citation:** Brosnan, K., Grun, B., & Dolnicar, S. (2021). "Cognitive load reduction strategies in questionnaire design." *International Journal of Market Research*, 63(2), 125-133.
**DOI:** https://doi.org/10.1177/1470785320986797

**Key findings:**
- Survey data quality suffers when respondents have difficulty completing complex tasks in questionnaires.
- Cognitive load reduction strategies from educational theory can potentially be used in questionnaire design.
- However, traditional survey answer formats such as **grid questions lead to equally good data** and do not frustrate respondents more than alternative formats.
- Reducing the number of items generally reduces cognitive load, increases engagement, reduces abandonment, and increases accuracy.

**Flow design implication:** The NPS survey flow's brevity (2 questions) is psychometrically well-supported from a cognitive load perspective. Any additions to the flow must be carefully weighed against increased cognitive burden and dropout risk.

---

#### Liu & Wronski (2018) — Completion Rates in 25,000+ Web Surveys

**Citation:** Liu, M. & Wronski, L. (2018). "Examining Completion Rates in Web Surveys via Over 25,000 Real-World Surveys." *Social Science Computer Review*, 36(1), 116-124.
**DOI:** https://doi.org/10.1177/0894439317695581

**Key findings:**
- Negative relationship between completion rate and survey length and question difficulty.
- **Surveys without progress bars have higher completion rates** than surveys with progress bars.
- The type and length of the first question significantly affect overall completion rate.

**Flow design implication:** The NPS question should be the FIRST question in the survey flow (it is simple and engaging). Progress bars should be avoided in short NPS surveys. Each additional step reduces completion by a measurable amount.

---

### 5.4 NPS-Specific Reviews

#### Dawes (2024) — What Should Managers Know

**Citation:** Dawes, J. G. (2024). "The net promoter score: What should managers know?" *International Journal of Market Research*, 66(2-3).
**DOI:** https://doi.org/10.1177/14707853231195003

**Key findings:**
- Canvasses four concerns: (1) its presumed link to business growth, (2) the assumption that low NPS scores indicate negative WOM, (3) the weak association between stated likelihood to recommend and actual recommending, and (4) the claim that NPS is a superior metric to customer satisfaction.
- When NPS questions are administered **verbally** (e.g., via call centers), "the gradations of agreement become too fine to easily express in words" on an 11-point scale.
- NPS counting method introduces additional statistical variation compared to simple averages.

**Flow design implication:** The survey delivery mode matters. Verbal/phone-based NPS surveys should consider using a shorter scale (e.g., 5-point) because 11 gradations are too fine for verbal administration.

---

#### Lacohee, Souchon, Dickenson, Krug & Saffre (2024) — Services Marketing Lens

**Citation:** Lacohee, H., Souchon, A., Dickenson, P., Krug, L., & Saffre, F. (2024). "The Net Promoter Score interrogated through a services marketing lens: Review and recommendations for service organizations." *International Journal of Market Research*, 66(2-3).
**DOI:** https://doi.org/10.1177/14707853231218605

**Key findings:**
- Identified **10 NPS interrogations** (weaknesses) through a services marketing framework.
- **Single-item measures obscure the complex nature of service evaluations**, whereas multidimensional measures help organizations identify what they are doing well and where improvements are needed.
- The combination of questionable research underpinning NPS and an under-researched nomological net creates a paradox: organizations strive to raise a score with little evidence-based knowledge on how to raise it strategically.

**Flow design implication:** For service organizations, the NPS survey flow should include service-specific dimensions (reliability, responsiveness, assurance, empathy, tangibles) to provide actionable diagnostic data alongside the overall NPS score.

---

#### Nunan (2024) — Two Decades of NPS Editorial

**Citation:** Nunan, D. (2024). "Two decades of Net Promoter Score: Relevance or evidence?" *International Journal of Market Research*, 66(2-3).
**DOI:** https://doi.org/10.1177/14707853241242228

**Key findings:**
- NPS has been adopted not just as a headline performance metric, but also as a tool for monitoring frontline staff and as part of customer management systems.
- The concept has developed over time, but the lively debate over the role and effectiveness of NPS continues.
- Reflects on the gap between academic skepticism and widespread managerial adoption.

**Flow design implication:** Despite academic concerns, NPS has practical value as a standardized benchmark. Survey flows should preserve NPS for comparability while enhancing it with additional measures.

---

#### Baehre (2024) — From Research to Action

**Citation:** Baehre, S. (2024). "From Research to Action: Enhancing Net Promoter Score Utilization in Managerial Practice." *International Journal of Market Research*, 66(2-3).
**DOI:** https://doi.org/10.1177/14707853231209893

**Key findings:**
- Distinguishes between **transactional NPS** (after specific interactions) and **brand health NPS** (periodic relationship surveys).
- Provides five essential managerial considerations for using NPS.
- Like any tool, NPS's effectiveness depends on appropriate and informed usage.

**Flow design implication:** Survey flows should be explicitly designed for either transactional or relationship measurement contexts, with different follow-up question structures for each.

---

#### East, Romaniuk & Lomax (2011) — Alternative to NPS

**Citation:** East, R., Romaniuk, J., & Lomax, W. (2011). "The NPS and the ACSI: A Critique and An Alternative Metric." *International Journal of Market Research*, 53(3), 327-346.
**DOI:** https://doi.org/10.2501/IJMR-53-3-327-346

**Key findings:**
- NPS does not measure **negative word of mouth** effectively.
- The ACSI (American Customer Satisfaction Index) is similarly insensitive to dissatisfaction.
- **Ex-customers and never-customers are not sampled** in these metrics — these are the people who express most of the negative sentiments.
- Proposed an alternative measuring the **volume and mean impact on purchase probability** of both positive and negative WOM.

**Flow design implication:** Survey flows that only sample current customers miss the most critical negative-WOM signal. The flow should ideally include a mechanism for capturing ex-customer sentiment, or at minimum, include a question about negative WOM propensity.

---

#### Raassens & Haans (2017) — NPS and Online Word-of-Mouth

**Citation:** Raassens, N. & Haans, H. (2017). "NPS and Online WOM: Investigating the Relationship Between Customers' Promoter Scores and eWOM Behavior." *Journal of Service Research*, 20(3), 322-334.
**DOI:** https://doi.org/10.1177/1094670517696965

**Key findings:**
- Promoter scores have a significant positive effect on online message valence.
- However, customers tend to spread **more negative than positive eWOM**, which differs from offline WOM research.
- The relationship between NPS categories and actual eWOM behavior varies.

**Flow design implication:** NPS captures recommendation intent but misses the asymmetric nature of online sharing behavior (where negative experiences drive more posting). Survey flows targeting digital products should include a specific online-sharing intent question.

---

#### Dixon, Freeman & Toman (2010) — Customer Effort Score as Alternative

**Citation:** Dixon, M., Freeman, K., & Toman, N. (2010). "Stop Trying to Delight Your Customers." *Harvard Business Review*, 88(7-8), 116-122.
**URL:** https://hbr.org/2010/07/stop-trying-to-delight-your-customers

**Key findings:**
- Study of more than 75,000 people interacting with contact-center representatives or using self-service channels.
- Over-the-top efforts to delight customers make little difference; customers primarily want a **simple, quick solution** to their problem.
- Introduced the **Customer Effort Score (CES)** and showed that it is a **better predictor of loyalty than customer satisfaction measures or the NPS**.
- Recommended five loyalty-building tactics: reducing repeat calls, addressing emotional needs, minimizing channel switching, using feedback from struggling customers, and focusing on problem solving over speed.

**Flow design implication:** For transactional/support contexts, CES may be a better primary metric than NPS. Survey flows could use CES as the lead question after service interactions ("How easy was it to resolve your issue?") with NPS reserved for periodic relationship measurement.

---

#### Lemon & Verhoef (2016) — Customer Experience Throughout the Journey

**Citation:** Lemon, K. N. & Verhoef, P. C. (2016). "Understanding Customer Experience Throughout the Customer Journey." *Journal of Marketing*, 80(6), 69-96.
**DOI:** https://doi.org/10.1509/jm.15.0420

**Key findings:**
- Customers interact with firms through myriad touchpoints in multiple channels and media.
- Identified four types of touchpoints: brand-owned, partner-owned, customer-owned, and social/external.
- Customer experience is **cumulative across all touchpoints** and stages (pre-purchase, purchase, post-purchase).
- A single NPS score captures only a snapshot and cannot diagnose which touchpoints are responsible.

**Flow design implication:** NPS survey flows should be deployed at **specific touchpoints** (transactional NPS) rather than only as a general relationship measure. Different touchpoints may require different follow-up questions. Mapping NPS results to the customer journey provides far more actionable insight than aggregate NPS alone.

---

### 5.5 NPS Gender and Demographic Bias

#### Gender Bias Research

**Citation:** Referenced in multiple studies including Pechter & Kuusik (2024) and the broader NPS literature.

**Key findings:**
- Women are **more likely to be Promoters** than men, and female Promoters are more likely to choose a score of 10.
- Men are **more likely to provide high Detractor ratings** (5 and 6).
- The NPS is heavily influenced by gender differences in the underlying score distribution.
- More educated, high-earning, and younger customers tend to score higher on NPS.

**Flow design implication:** NPS scores should be analyzed with demographic controls. Survey flows that collect demographic data (even minimal: age, gender) enable more nuanced interpretation and can help distinguish genuine product/service issues from demographic composition effects.

---

### 5.6 Healthcare-Specific NPS Validation

#### Adams, Walpola, Schembri & Harrison (2022) — Systematic Review in Healthcare

**Citation:** Adams, C., Walpola, R., Schembri, A. M., & Harrison, R. (2022). "The ultimate question? Evaluating the use of Net Promoter Score in healthcare: A systematic review." *Health Expectations*, 25(5), 2328-2339.
**DOI:** https://doi.org/10.1111/hex.13577

**Key findings:**
- There has been little research to validate NPS effectiveness and value in healthcare settings.
- NPS may not be sufficient as a **stand-alone metric** and may be better used in conjunction with a larger survey.
- NPS may be more suited for use where **patients have a choice of provider**.
- Many claims initially made to promote NPS (ability to predict customer loyalty) were not supported by subsequent research.

**Flow design implication:** In healthcare and other contexts where users have limited choice, the recommendation question is less meaningful. Survey flows should substitute or supplement with satisfaction and effort measures.

---

## 6. The "Close the Loop" Step

### 6.1 Service Recovery and Complaint Response

#### Knox & Van Oest (2014) — Financial Impact of Complaint Recovery

**Citation:** Knox, G. & Van Oest, R. (2014). "Customer Complaints and Recovery Effectiveness: A Customer Base Approach." *Journal of Marketing*, 78(5), 42-57.
**DOI:** https://doi.org/10.1509/jm.12.0317

**Key findings:**
- Complaints are associated with a **substantial increase** in the probability that a customer stops buying.
- Prior purchases **mitigate** the churn effect and their impact is long-lasting, whereas prior complaints **exacerbate** the effect but their impact is short-lived.
- The **financial benefits of handling customer complaints outweigh the costs** in most recovery scenarios.

**Flow design implication:** The "close the loop" step (contacting detractors) is financially justified. Survey flows should be designed to enable rapid identification and routing of detractor responses to frontline teams. The flow should capture enough context (score + reason) to enable effective recovery.

---

#### Gelbrich & Roschk (2011) — Meta-Analysis of Complaint Handling

**Citation:** Gelbrich, K. & Roschk, H. (2011). "A Meta-Analysis of Organizational Complaint Handling and Customer Responses." *Journal of Service Research*, 14(1), 24-43.
**DOI:** https://doi.org/10.1177/1094670510387914

**Key findings:**
- Recovery counters the effect of the complaint but, in almost all cases, does **not entirely offset** it (the "service recovery paradox" is not universal).
- Effective complaint handling includes: (1) encouraging complaints, (2) establishing dedicated handling teams, (3) resolving problems quickly, (4) developing a complaint database, (5) identifying failure points, (6) tracking trends.

**Flow design implication:** The NPS close-the-loop process should set realistic expectations — recovery improves outcomes but rarely restores full pre-complaint loyalty. The survey flow should facilitate rapid response (within hours, not days) for maximum recovery effect.

---

### 6.2 Feedback to Respondents

#### Wenz, Jackle, Burton & Couper (2022) — Personalized Feedback Effects

**Citation:** Wenz, A., Jackle, A., Burton, J., & Couper, M. P. (2022). "The Effects of Personalized Feedback on Participation and Reporting in Mobile App Data Collection." *Social Science Computer Review*, 40(1), 165-178.
**DOI:** https://doi.org/10.1177/0894439320914261

**Key findings:**
- Offering personalized feedback on survey results **increases response rates** by approximately 7 percentage points.
- The effect is particularly strong for respondents who find the survey topic less salient.
- However, personalized feedback **might lead participants to change the behavior being measured**, is costly to implement, and constrains design decisions.
- **Aggregated feedback** (sharing overall study results) does not increase participation rates and can even have negative effects, while **personalized feedback** mostly reports positive but modest effects.

**Flow design implication:** The NPS thank-you/close screen could include personalized feedback (e.g., "Thank you! Your feedback will be used to improve X.") to boost future response rates. However, showing aggregate results (e.g., "Our current NPS is X") may not help and could even reduce future participation.

---

### 6.3 Social Exchange Theory and Survey Reciprocity

#### Dillman (2014) — Tailored Design Method

**Citation:** Dillman, D. A., Smyth, J. D., & Christian, L. M. (2014). *Internet, Phone, Mail, and Mixed-Mode Surveys: The Tailored Design Method* (4th ed.). Wiley.

**Key findings:**
- Survey participation is governed by **social exchange theory**: respondents weigh rewards, costs, and trust.
- **Reciprocity norms** — giving something before asking (e.g., advance notice, explanation of purpose, small incentive) — increase response rates.
- Multiple contacts (advance notice, initial survey invitation, reminders, thank-you) significantly improve participation.
- Trust in the survey sponsor and clarity of purpose are essential motivators.

**Flow design implication:** The NPS survey flow should embed social exchange principles: (1) Clear explanation of why the survey matters, (2) Minimal time cost (2-3 questions), (3) Acknowledgment/thank-you that communicates value ("Your response helped us improve X"), (4) Follow-up that demonstrates responsiveness.

---

### 6.4 Sharing Results Back to Respondents

#### MacDuffie (2025) — Obligation to Share Aggregate Results

**Citation:** MacDuffie, K. E. et al. (2025). "Revisiting the Obligation to Share Aggregate Results with Research Participants in the Era of Open Science." *Ethics & Human Research*.
**DOI:** https://doi.org/10.1002/eahr.60009

**Key findings:**
- Research participants typically report a desire to **help others** (altruistic tendency) as a top motivation for participating.
- Motivation to help is **stronger when people can expect feedback** about how their actions helped.
- Communicating aggregate results can fulfill the **altruistic expectations** that motivate participation and potentially even **encourage future participation**.
- However, 30.8% of researchers reported a **lack of institutional incentive** for sharing results with participants.

**Flow design implication:** The NPS close/thank-you screen should communicate how the respondent's feedback will be used. Subsequent communications should share what changed as a result of feedback ("You told us X, so we improved Y"). This fulfills the social exchange reciprocity that drives future response rates.

---

### 6.5 Proactive vs. Reactive Complaint Management

#### Nuansi & Ngamcharoenmongkol (2021) — Proactive Complaint Management

**Citation:** Nuansi, P. & Ngamcharoenmongkol, P. (2021). "Proactive Complaint Management: Effects of Customer Voice Initiation on Perceived Justices, Satisfaction, and Negative Word-of-Mouth." *SAGE Open*, 11(4).
**DOI:** https://doi.org/10.1177/21582440211040788

**Key findings:**
- Proactive complaint management (company initiating the feedback conversation rather than waiting for customer to complain) affects perceived justices, satisfaction, and negative WOM.
- When the company initiates the voice (as in an NPS survey triggering follow-up), customers perceive higher procedural justice.

**Flow design implication:** The NPS survey itself, when followed by company-initiated contact, functions as proactive complaint management. This positions the NPS flow not just as measurement but as a service recovery tool. Detractor follow-up should be framed as "We noticed from your feedback..." rather than waiting for customers to escalate.

---

---

## Summary: Academically-Informed NPS Survey Flow

Based on the totality of academic evidence reviewed (40+ peer-reviewed sources), here is what an optimal NPS survey flow would look like:

### Pre-Survey
- Clear, personalized invitation explaining purpose and estimated time (Dillman, 2014)
- Appropriate timing relative to interaction — transactional NPS near the touchpoint, relationship NPS at regular intervals (Baehre, 2024; Pechter & Kuusik, 2024)
- Context matters: for support interactions, consider CES as lead metric instead (Dixon et al., 2010)

### Step 1: NPS Score Question
- **Input format:** Use discrete radio buttons or clickable number buttons, NOT sliders (Bosch et al., 2019; Funke et al., 2011)
- **Layout:** Horizontal is standard; vertical may reduce burden for certain populations (Hu, 2020). Use ascending order (0 to 10, left to right) for higher validity (Chyung, 2018)
- **Labeling:** Fully label at least endpoints; consider labeling all points for better measurement quality (Menold, 2020; DeCastellarnau, 2018). Avoid numeric-only labels
- **Scale length:** The 11-point (0-10) scale exceeds the psychometric optimum of 5-7 points (Preston & Colman, 2000) and is too fine for verbal administration (Dawes, 2024). Consider 7-point alternative for better reliability
- **Cross-cultural deployment:** Consider 5-point or 7-point scale to reduce cultural response style effects (Beuthner et al., 2018; Harzing, 2006). In collectivist cultures (Japan, Korea), supplement or replace recommendation intent with satisfaction question (Seth et al., 2016)
- **Placement:** If this is the only question needing an unbiased overall reading, place it FIRST. If identifying drivers, place AFTER specific attribute questions (Thau, 2021; van de Walle & van Ryzin, 2011)

### Step 2: Conditional Follow-Up (varies by score)
- Open-ended responses alone have **low to moderate convergence** with actual score drivers (Bettencourt & Houston, 2024). Supplement with structured questions.
- **For Detractors (0-6):** Ask "What is the primary reason for your score?" (open-ended) + "Which of these areas most needs improvement?" (closed-ended checklist). Detractors show the highest convergence between open-ended and actual priorities (Bettencourt & Houston, 2024)
- **For Passives (7-8):** Ask "What would we need to do to earn a higher score?" (open-ended)
- **For Promoters (9-10):** Ask "What do you value most about us?" (open-ended). Note: open-ended priorities from promoters have the lowest convergence with actual drivers (Bettencourt & Houston, 2024)
- Make open-ended questions **optional** to reduce dropout (Hadler, 2025). Offering pre-coded reason categories (with an "other" option) as a first step can reduce cognitive burden while preserving the ability to capture novel themes

### Step 3 (Optional but Recommended): Supplementary Metric
- **Multi-metric approaches outperform single NPS** (De Haan et al., 2015; Morgan & Rego, 2006; Keiningham et al., 2007, 2008)
- Add one of: CSAT ("Overall, how satisfied are you?"), CES ("How easy was it to [complete task]?"), or an emotion-capture question (Net Emotional Value) (Muller et al., 2024)
- Average customer satisfaction is the **strongest predictor** of business performance in most studies (Morgan & Rego, 2006; De Haan et al., 2015)
- Keep total survey to **3-4 questions maximum** to preserve completion rates (Liu & Wronski, 2018; Brosnan et al., 2021). Each additional question increases dropout by approximately 2% per 100 items (survey length research)
- Do NOT show progress bars for short surveys (Liu & Wronski, 2018)

### Step 4: Thank-You / Close
- Provide **personalized** acknowledgment, not aggregated results — aggregated feedback can reduce future participation (Wenz et al., 2022)
- Communicate **specifically** that their feedback will be acted upon ("We'll use your feedback to improve X") (Dillman, 2014; MacDuffie, 2025)
- For detractors: Enable rapid **frontline follow-up within hours**, framed as proactive outreach (Knox & Van Oest, 2014; Gelbrich & Roschk, 2011; Nuansi & Ngamcharoenmongkol, 2021)
- Set realistic expectations: recovery improves outcomes but rarely restores full pre-complaint loyalty (Gelbrich & Roschk, 2011)
- In subsequent communications, **share what changed** as a result of feedback to fulfill reciprocity norms and encourage future responses (MacDuffie, 2025)

### Post-Survey Analysis
- Apply **NLP/sentiment analysis** to open-ended responses for validation and theme extraction (Lewis & Mehmet, 2020; Tarnowska & Ras, 2019)
- Use **mean LTR score** alongside traditional NPS for more statistically stable tracking (Eskildsen & Kristensen, 2011; Fisher, 2019)
- **Cross-validate** with behavioral data: actual recommendation behavior, retention, spending (Reichheld, 2021; Raassens & Haans, 2017; Romaniuk et al., 2011)
- Analyze results **with demographic controls** (gender, age) as NPS is subject to demographic bias (gender bias research; Jaramillo et al., 2024)
- For international deployments, apply **cultural response style corrections** or analyze within-country trends only (Harzing, 2006; Seth et al., 2016)

---

## Appendix: Complete Citation Index

### Foundational NPS Works
1. Reichheld, F.F. (2003). "The One Number You Need to Grow." *Harvard Business Review*, 81(12), 46-54.
2. Reichheld, F.F. & Markey, R. (2011). *The Ultimate Question 2.0.* Harvard Business Press.
3. Reichheld, F.F., Darnell, D. & Burns, M. (2021). "Net Promoter 3.0." *Harvard Business Review*, 99(6), 38-47.

### Academic Critiques (Scale, Validity, Methodology)
4. Grisaffe, D.B. (2007). *Journal of Consumer Satisfaction, Dissatisfaction and Complaining Behavior*, 20, 36-53.
5. Keiningham, T.L. et al. (2007). *Journal of Marketing*, 71(3), 39-51.
6. Keiningham, T.L. et al. (2008). *Journal of Database Marketing & Customer Strategy Management*, 15(2), 79-90.
7. Morgan, N.A. & Rego, L.L. (2006). *Marketing Science*, 25(5), 426-439.
8. Eskildsen, J.K. & Kristensen, K. (2011). Conference paper on NPS reliability.
9. East, R., Romaniuk, J. & Lomax, W. (2011). *International Journal of Market Research*, 53(3), 327-346.
10. Romaniuk, J., Nguyen, C. & East, R. (2011). *International Journal of Market Research*, 53(4), 507-521.
11. Bendle, N.T. & Bagga, C.K. (2016). *MIT Sloan Management Review*, 57(3), 73-82.
12. Fisher, N.I. & Kordupleski, R.E. (2019). *Applied Stochastic Models in Business and Industry*, 35(1), 138-151.

### 2024 Special Issue — International Journal of Market Research
13. Nunan, D. (2024). *IJMR*, 66(2-3). DOI: 10.1177/14707853241242228
14. Dawes, J.G. (2024). *IJMR*, 66(2-3). DOI: 10.1177/14707853231195003
15. Schlosser, A. (2024). *IJMR*, 66(2-3). DOI: 10.1177/14707853231186309
16. Bettencourt, L.A. & Houston, M.B. (2024). *IJMR*, 66(2-3). DOI: 10.1177/14707853231198780
17. Jaramillo, S. et al. (2024). *IJMR*, 66(2-3). DOI: 10.1177/14707853231213274
18. Lacohee, H. et al. (2024). *IJMR*, 66(2-3). DOI: 10.1177/14707853231218605
19. Muller, S., Seiler, R. & Volkle, M. (2024). *IJMR*, 66(2-3). DOI: 10.1177/14707853231219648
20. Baehre, S. (2024). *IJMR*, 66(2-3). DOI: 10.1177/14707853231209893
21. Pechter, J. & Kuusik, A. (2024). *IJMR*, 66(2-3). DOI: 10.1177/14707853231214188
22. Maubisson, L., Mencarelli, R. & Riviere, A. (2024). *IJMR*, 66(2-3). DOI: 10.1177/14707853231219422

### NPS and Word-of-Mouth
23. Raassens, N. & Haans, H. (2017). *Journal of Service Research*, 20(3), 322-334.
24. Dixon, M., Freeman, K. & Toman, N. (2010). *Harvard Business Review*, 88(7-8), 116-122.

### Customer Feedback Metric Comparisons
25. De Haan, E., Verhoef, P.C. & Wiesel, T. (2015). *International Journal of Research in Marketing*, 32(2), 195-206.
26. Lemon, K.N. & Verhoef, P.C. (2016). *Journal of Marketing*, 80(6), 69-96.

### Psychometric / Scale Design
27. Bergkvist, L. & Rossiter, J.R. (2007). *Journal of Marketing Research*, 44(2), 175-184.
28. Diamantopoulos, A. et al. (2012). *Journal of the Academy of Marketing Science*, 40(3), 434-449.
29. Preston, C.C. & Colman, A.M. (2000). *Acta Psychologica*, 104(1), 1-15.
30. DeCastellarnau, A. (2018). *Quality & Quantity*, 52(4), 1523-1559.
31. Menold, N. (2020). *Sociological Methods & Research*, 49(1), 79-107.

### Survey UX Design
32. Bosch, O.J. et al. (2019). *Social Science Computer Review*, 37(1), 119-139.
33. Hu, J. (2020). *Social Science Computer Review*, 38(6), 741-759.
34. Chyung, S.Y. et al. (2018). *Performance Improvement*, 57(3), 9-16.
35. Liu, M. & Wronski, L. (2018). *Social Science Computer Review*, 36(1), 116-124.
36. Hadler, P. (2025). *Sociological Methods & Research*, 54(1).

### Cognitive Load and Survey Methodology
37. Brosnan, K., Grun, B. & Dolnicar, S. (2021). *International Journal of Market Research*, 63(2), 125-133.
38. Dillman, D.A. et al. (2014). *The Tailored Design Method* (4th ed.). Wiley.
39. Thau, M. (2021). *Public Administration*, 99(1), 132-146.
40. van de Walle, S. & van Ryzin, G. (2011). *Public Administration*, 89(4), 1436-1450.

### Cross-Cultural Research
41. Harzing, A.-W. (2006). *International Journal of Cross-Cultural Management*, 6(2), 243-266.
42. Beuthner, C. et al. (2018). *International Journal of Market Research*, 60(1), 33-48.
43. Seth, S. et al. (2016). *Asia Marketing Journal*, 17(4), 43-61.

### AI/NLP and NPS
44. Lewis, C. & Mehmet, M. (2020). *International Journal of Market Research*, 62(1), 9-21.
45. Tarnowska, K. & Ras, Z.W. (2019+). Multiple publications, UNC Charlotte.
46. Kim, S., Lee, J. & Gweon, G. (2019). *ACM CHI Conference Proceedings.*

### Close the Loop / Service Recovery
47. Knox, G. & Van Oest, R. (2014). *Journal of Marketing*, 78(5), 42-57.
48. Gelbrich, K. & Roschk, H. (2011). *Journal of Service Research*, 14(1), 24-43.
49. Wenz, A. et al. (2022). *Social Science Computer Review*, 40(1), 165-178.
50. MacDuffie, K.E. et al. (2025). *Ethics & Human Research*.
51. Nuansi, P. & Ngamcharoenmongkol, P. (2021). *SAGE Open*, 11(4).

### Healthcare NPS
52. Adams, C. et al. (2022). *Health Expectations*, 25(5), 2328-2339.
